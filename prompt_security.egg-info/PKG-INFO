Metadata-Version: 2.1
Name: prompt_security
Version: 0.0.1
Summary: A short description of the project.
Home-page: 
Author: Your name (or your organization/company/team)
Author-email: 
License: 
        The MIT License (MIT)
        Copyright (c) 2024, Your name (or your organization/company/team)
        
        Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
        
        
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE

# prompt-security

Prompt injection is a technique used to manipulate language models or AI systems by providing specially crafted input that causes the system to produce unintended or harmful responses. This form of attack leverages the model's reliance on user input to steer its behavior, potentially leading to security vulnerabilities, misinformation, or other detrimental outcomes. Understanding and mitigating prompt injection is crucial for ensuring the safe and reliable deployment of AI technologies.


Today, companies such [cloudflare](https://blog.cloudflare.com/firewall-for-ai)  offer solutions to detect prompt injections attacks , and interest in protection keeps increasing


This project offers an evaluation of 3 types of detectors

1. Perplexity based detector
2. Classical nlp based detector
3. Embedding based detector


Moreover, we evaluate the strengths and weaknesses of each type of detector ,specfilally in relation to the following attack types:
* [GCG](https://arxiv.org/pdf/2307.15043)
* Peraphrase attack 


# Datasets
This project uses 3 datasets of adverserial samples:

* Peraphrase attack generation dataset : we use as a base couple of prompt injections and we generate variations using different  detection methods.The dataset can be found at : https://www.kaggle.com/datasets/arielzilber/prompt-injection-peraphrase-attack

* Suffix dataset : we follow the   [GCG](https://arxiv.org/pdf/2307.15043) paper and generate a suffix attack dataset.The dataset can be found at : https://www.kaggle.com/datasets/arielzilber/prompt-injection-suffix-attack

* Known prompt injection datasets : we gathered all the exisiting adverserial samples datasets avaliable on kaggle and on huggingface.The dataset can be found at : https://www.kaggle.com/datasets/arielzilber/prompt-injection-in-the-wild


And the following benign dataset:

https://www.kaggle.com/datasets/arielzilber/prompt-injection-benign-evaluation-framework

# Notebooks

1. [detecting language model attack](/notebooks/detecting_language_model_attack.ipynb)

2. [generate adverserial suffix dataset](/notebooks/generate_adverserial_suffix_dataset.ipynb)

3. [perphrase attack generation](/notebooks/perphrase_attack_generation.ipynb)

4. [dataset preproccessing](/notebooks/dataset_preproccessing.ipynb)
